# Gradient Descent

* * *

## 미분
* 변화율($f(x+h) - f(x)$)의 극한
* 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구 -> 최적화에서 많이 사용
* (x, $f(x)$) 에서의 접선의 기울기를 구할 수 있음.

> <img width="200" alt="image" src="https://user-images.githubusercontent.com/93971443/191641281-08bcd8c8-81ba-4630-a2c8-3aff00a4aff5.png">

* 접선의 기울기를 안다면 점이 어느 방향으로 움직여야 함수값이 증가 / 감소하는지 알 수 있다.
  - 증가 : 점선의 기울기 양/음수 상관없이 더해줌 --> 경사 상승법 --> 함수의 극대값으로 수렴
  - 감소 : 접선의 기울기 양/음수 상관없이 빼줌 --> 경사 하강법 --> 함수의 극소값으로 수렴
  - 경사상승/하강법은 극점에 도달하면 종료됨 --> 미분값이 0이면 더 이상 업데이트가 안되기 때문 

> > <img width="657" alt="image" src="https://user-images.githubusercontent.com/93971443/191642378-9dfa78a6-79be-4dc2-8bce-97c9b06a70f4.png">

* 코드를 통한 미분 계산

<pre>
<code>
import sympy as sym
from sympy.abc import x

sym.diff(sym.poly(x**2 + 2*x + 3),x)
</code>
</pre>


* * *

### 다변량 함수에서의 미분
* 편미분을 통해 각 변수 방향에서의 변화율을 얻는다. --> gradient vector 얻을 수 있음

> <img width="292" alt="image" src="https://user-images.githubusercontent.com/93971443/191642587-6c1a1c22-9d28-440d-8e18-a706faf04743.png">

* 구한 gradient vector를 이용하여 경사하강/상승법에 사용

* * *

## 경사하강법 알고리즘 - 1

* 단량 함수
<pre>
<code>
var = input # 시작점
grad = gradient(var)
while (abs(grad) > eps): # 종료조건
  var = car - lr * grad # lr(학습률) : 수렴속도 결정
  grad = gradient(var)
</code>
</pre>
  
  
* 다변량 함수 --> 절대값을 적용할 수 없으니 norm을 통해 계산한다.
<pre>
<code>
var = input # 시작점
grad = gradient(var)
while (norm(grad) > eps): # 종료조건
  var = car - lr * grad
  grad = gradient(var)
</code>
</pre>

* * 코드로 경사하강법 진행시 미분의 컴퓨터 연산이 정확히 0을 갖기 어려움으로 종료 조건을 설정해주어야한다. (eps 활용하여 종료조건 생성)

* * *

### 경사하강법으로 선형회귀 계수 구하기
> 무어-팬로즈 역행렬을 이용하여 구할 수도 있지만 경사하강법 사용하는 이유
>   - 머신러닝에서는 비선형 모델도 목적함수가 되는데 모델에 잘 적용하기 위함

* 선형회귀 목적식 : $||y - X\beta||$ 
* 이를 최소화하는 $\beta$를 찾기 위해 그래디언트 벡터를 구하면 다음과 같다.

> <img width="600" alt="image" src="https://user-images.githubusercontent.com/93971443/191648097-c608eb17-4fed-499b-b43c-0b052186bba6.png">

* 경사하강법 알고리즘

> <img width="600" alt="image" src="https://user-images.githubusercontent.com/93971443/191648248-3206f869-1f69-4cb3-8f7c-c111c15bc2c3.png">

* 위처럼 구하기도 하지만 간단한 연산을 위해 L2-norm의 제곱을 이용한다.(결과는 같음)

> <img width="600" alt="image" src="https://user-images.githubusercontent.com/93971443/191648533-fcd98d3e-17ef-4017-a317-e4567c0d5f44.png">

* * *

### 경사하강법 알고리즘 - 2

<pre>
<code>
for t in T: # T: 학습횟수
  error = y - X @ beta
  grad = - transpose(X) @ error
  beta = beta - lr * grad
</code>
</pre>

위 경사하강법 알고리즘 - 1과 과정은 동일 단, 종료 조건을 학습횟수로 진행한다. 

* * * 

### 경사하강법은 만능인가?

* 경사하강법은 이론적으로 미분가능하고 볼록(convex)한 함수에 대해서 수렴이 보장
* 비선형회귀의 경우 convex한 형태가 아니기 때문에 수렴이 보장되지 않는다. 이 경우 확률적 경사하강법(Stochastic Gradient Descent 이하 SGD)를 사용

##### 확률적 경사하강법

* 모든 데이터를 사용하는 대신 한개 또는 일부 활용
* 하나를 사용할 경우 SGD / 일부를 사용할 경우 mini-batch SGD라 한다.
* 전체 데이터 : n / 배치 사이즈 : b 일 때, 연산의 양이 b/n 만큼 줄어든다. -> 효율적

* 미니 배치의 구성은 확률적으로 선택하기 때문에 목적식의 모양이 바뀌게 된다. -> non-convex 형태일 때 극소점 탈출가능!

> <img width="600" alt="image" src="https://user-images.githubusercontent.com/93971443/191657548-816f86d5-9bfe-49f1-a08f-becf0830a89f.png">


###### SGD를 활용하면 데이터를 메모리에 다 올리지 않기 때문에 빠른 연산과 하드웨어 한계를 해결할 수 있다.


### ※ 위 내용 및 이미지는 네이버 부스트캠프 교육 자료를 참고하였습니다.
