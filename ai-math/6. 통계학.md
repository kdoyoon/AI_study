# 통계학

* * *

## 모수
* 정의 : 평균, 분산과 같은 모집단의 데이터
* 통계적 모델링은 적절한 가정을 통해 확률분포를 추정 그러나 유한 개의 데이터로 완벽한 추정 불가능
* 위의 이유로 근사적 확률분포 추정시 모수 사용
* 모수적 방법론 : 특정 확률분포를 따른다고 가정 후 모수 추정
* 비모수적 방법론 : 특정 확률분포라 가정하지 않고 모수 추정(모수가 없는 것이 아닌 개수가 무한개/가변적)

* * *

## 확률분포 가정하기
* 여러 조건을 보고 기계적 확률분포 가정 지양
* 데이터를 생성하는 원리를 먼저 고려!

* * * 

## 데이터로 모수 추정

* 예시. 정규분포 : 모수 - 평균, 분산(*-> 표본분산은 불평추정량에 의해 N-1로 나눈다.)

<img width="600" alt="image" src="https://user-images.githubusercontent.com/93971443/191930914-4f5af082-fbd6-4bdb-8187-8cefcb93e69e.png">

* 통계량의 확률분포 -> 표집분포(sampling distribution)이라 부름 (* 표집분포는 N이 커질수록 정규분포를 따른다_중심극한정리)
> sample distribution VS sampling distribution
> * sample distribution(표본분포)
>   * 모집단에서 표본 선택
>   * 모집단이 정규분포를 따르지 않으면 표본분포는 정규분포를 따를 수 없다.
> * sampling distribution(표집분포)
>   * 통계량의 확률분포 
>   * 무한히 많이 반복시 정규분포를 따른다.(n이 커질 수록 분산이 0에 가까워지기 때문)

* * * 

## 최대가능도 추정법
* 이론적으로 가장 가능성이 높은 모수를 추정하는 방법
* 가능도함수는 모수 $/theta$를 따르는 분포 x가 관찰될 가능성(but, 확률로 해석X)

<img width="600" alt="image" src="https://user-images.githubusercontent.com/93971443/191933007-e5881ef0-e629-4496-a203-3a9012b1d5e2.png">

* 데이터가 독립적으로 추출되었을 경우 로그가능도 사용 가능
<img width="750" alt="image" src="https://user-images.githubusercontent.com/93971443/191933573-d15aae62-d36b-4b72-abce-492a2fe225e0.png">

> ## 로그가능도를 사용하는 이유
> * 데이터 숫자 단위를 줄여줌
> * 확률 값의 범위 확장해줌(-> 데이터 비교 용이 / NLP분야에서 많이 활용)
> * 곱셈 -> 덧셈 : 연산을 간단하게 해줌

* * *

## 확률분포의 거리
* 기계학습에서 사용되는 손실함수들은 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포의 거리를 통해 유도
* 예시
  * 쿨백-라이블러 발산
  <img width="750" alt="image" src="https://user-images.githubusercontent.com/93971443/191934346-943e8ba2-b0e8-497e-b86a-7de44891566d.png">
  
  * 분류문제 -> 최대가능도 추정법 : 쿨백-라이블러 발산을 최소화
> > 엔트로피 : 할 수 있는 최소의 데이터 크기 / 크로스 엔트로피 : 비효율적 크기 

* * *

# 베이즈 통계학

* * *

## 조건부 확률
* $P(A|B)$ : B 사건이 일어났을 때 A 사건이 일어날 확률

* * *

## 베이즈 정리
* 조건부 확률을 이용한 정보 갱신
* 새로운 정보 A가 주어졌을 때 $P(B)$로 $P(B|A)$를 계산하는 방법
<img width="600" alt="image" src="https://user-images.githubusercontent.com/93971443/191935620-822b22de-566d-417b-bb3a-f0cd34b16fe5.png">

* 예시
<img width="600" alt="image" src="https://user-images.githubusercontent.com/93971443/191935957-78b2105d-8b03-4aee-b67c-b385bc3e52f7.png">

*사후확률 : 데이터가 주어졌을 때 가설이 통과할 확률 \
*사전확률 : 주어진 확률(데이터 주어지지X) \
*가능도 : 현재 주어진 모수, 가정에서 $D$ 데이터가 관측될 확률 \
*Evidence : 데이터 자체의 분포

* 가능도를 통해 evidence를 구할 수 있음
<img width="350" alt="image" src="https://user-images.githubusercontent.com/93971443/191937304-6041d114-faef-4fc1-959d-25b7a7db3704.png">

### Confusion Matrix

<img width="400" alt="image" src="https://user-images.githubusercontent.com/93971443/191938094-ebb7766a-4708-4f7a-85d1-26aa39e23008.png">

* 1종 오류 : 맞는데 아니라 함(FN에 해당)
* 2종 오류 : 아닌데 맞다고 함(FP에 해당)
  * 데이터의 종류에 따라 어떤 오류를 줄일지 선택하게 됨

## 베이즈 정리를 통한 정보의 갱신
* 이전 과정에서 계산한 사후 확률을 사전 확률로 사용하여 갱신 가능
<img width="650" alt="image" src="https://user-images.githubusercontent.com/93971443/191938954-35f95317-e478-4b85-8dd3-692148c246bf.png">

## 조건부 확률과 인과관계
* 유용한 통계적 해석을 하지만 인과관계 추론시 함부로 사용하면 안됨
* 인과관계는 유입되는 데이터 모형이 다른 경우가 많을 경우 강건한 예측 모형을 만들때 사용(-> 예측 정확도가 크게 변하지 않아야 함)
* 무작정 조건부확률 적용시 가짜 연관성에 의해 결과의 정확도 하락
* 인과관계 도출을 위해 중첩 요인을 제거해야 함! 



