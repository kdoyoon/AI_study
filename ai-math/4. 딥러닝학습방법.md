# 딥러닝 학습방법

* * *

### 신경망
* 선형 모델과 활성 함수(activation function)을 합성한 함수
* 선형모델

> <img width="600" alt="image" src="https://user-images.githubusercontent.com/93971443/191732071-447952bc-a348-4434-8e8a-12ce2eb0163e.png">

* 활성 함수 (ex. 소프트맥스)

> <img width="600" alt="image" src="https://user-images.githubusercontent.com/93971443/191732534-5d4dbc7d-6131-4880-adbe-5622fda5fe70.png">
>
> *softmax함수 : 출력벡터를 입력하면 특정 클래스에 속할 확률을 반환한다. 보통 분류 문제를 풀 때 사용(학습시에만 사용, 추론시 사용X)
<pre>
<code>

def softmax(vec):
  denumerator = np.exp(vec - np.max(vec, axis - -1, keepdims = True))
  numerator = np.suim(denumerator, axis = -1, keepdims = True)
  val = denumerator / numerator
  return val
</code>
</pre>

* * * 

### 활성함수
* 비선형 함수로서 딥러닝 학습시 매우 중요!
* 만약 활성함수를 사용하지 않으면 딥러닝은 선형함수와 다르지 않게됨
* 종류
* * sigmoid
* * tanh
* * ReLU(-> 딥러닝에서 가장 많이 사용함)

<img width="750" alt="image" src="https://user-images.githubusercontent.com/93971443/191734270-755aeee9-3e13-4246-86af-43f678c3bb00.png">

* * *

### MLP
* 여러 층의 선형 함수, 활성함수를 거쳐 출력을 낸다.
* 순전파(forward propagation) : 학습이 아닌 단순 입력에 대한 output을 내는 과정

<img width="300" alt="image" src="https://user-images.githubusercontent.com/93971443/191735965-e4947ee3-482f-479e-a40c-f17ea4537b9f.png">

* 여러 층의 신경망을 쌓는 이유
* * 목적함수를 근사하는데 필요한 뉴런의 숫자가 빨리 줄어듬. 효율적 학습 가능
* * 적은 노드로 복잡한 패턴을 표현 가능해짐

* * *

### 딥러닝 학습 원리
###### 역전파(Backpropagtion)
* 각 층에서 사용된 파라미터(가중치, 절편)을 이용하여 학습
* 윗층에서부터 역순으로 계산됨. 이때, 미분의 연쇄법칙 기반 자동 미분을 사용

<img width="700" alt="image" src="https://user-images.githubusercontent.com/93971443/191738265-43d7d370-3e90-4c4f-8512-de255014f50c.png">



### (참고)

## 비선형 활성함수를 사용하는 이유

* 선형함수를 아무리 여러 층을 쌓아도 하나의 선형함수와 차이가 없음
* 다양한 패턴 나타내기에 한계가 있음

## ReLU가 가장 많이 사용되는 이유
* 다른 활성함수의 경우 양 끝점을 향할수록 기울기가 0에 가까워진다.
* 따라서 역전파 연산을 진행할수록 기울기가 점점 작아져 뒤에 있는 층까지 전달되지 않게된다.(이를 gradient vanishing이라함)

* ReLU 또한 음수일 경우 기울기가 0이라는 문제가 있어 리키 렐루(Leaky ReLU)를 사용하기도 한다.


### ※ 위 내용 및 이미지는 네이버 부스트캠프 교육 자료를 참고하였습니다.
