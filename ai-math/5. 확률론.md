# 확률론

* * *
## 확률분포
* 확률 변수가 특정한 값을 가질 확률을 나타내는 함수


## 확률변수
확률분포에 따라 종류가 나뉨
* 이산형(Discrete) -> 셀 수 있음
  * 확률변수가 가질 수 있는 경우의 수 모두 고려하여 확률을 다 더함(확률질량함수)

<img width="400" alt="image" src="https://user-images.githubusercontent.com/93971443/191892734-0969c8ad-d0a3-46b9-82f6-5d701ea82ec2.png">

* 연속형(Continuous) -> 셀 수 없음
  * 데이터 공간에 정의된 확률변수의 밀도 위에서의 적분을 함(확률밀도함수)
  * 확률로 해석하면 안됨

<img width="390" alt="image" src="https://user-images.githubusercontent.com/93971443/191892926-c1fc0904-e3ab-4692-b83d-1b2875e0549a.png">

* * *

## 결합분포
* 확률변수 2개 이상
* 확률분포 모델링시 원래 분포에 상관없이 모델링 방법에 따라 연속/이산이 달라짐
* 주변확률 분포를 유도할 때 사용됨
* $P(x,y)$로 표시


## 주변확률분포
* 결합분포를 하나의 변수로 표현하고 싶을 때 사용
* $P(X)$ : x에 대한 주변확률분포, y에 대한 정보 제공 X
* x에 대한 주변확률분포 구할시 결합분포 $P(x,y)$에 각 y에 대해 더해주거나(이산), y에 대해 적분(연속)을 통해 유도
<img width="500" alt="image" src="https://user-images.githubusercontent.com/93971443/191894801-53cada2f-ef24-41e1-97d0-b7f2f3577baa.png">


## 조건부 확률
* $P(x|y)$ : 입력변수 x에 대해 정답이 y일 확률을 의미
* 특정 클래스가 주어진 조건에서 데이터의 롹률분포 보여줌 --> 통계적 관계 모델링 / 예측모형 모델링시 사용

* 기계학습에서의 조건부 확률
  * $P(y|x)$ :입력변수 x에 대해 정답이 y일 확률
  * 분류 문제
    * 로지스틱 회귀 : 선형모델 + softmax함수의 결합 --> 확률해석
    * softmax 함수는 이전 레이어들에서 추출된 데이터 패턴과 가중치를 통해 조건부 확률 $P(y|x)$을 계산
  
  * 회귀문제
    * 확률해석에 어려움이 있다.(밀도로 해석해야함)
    * 조건부 기대값을 사용(L2-norm을 최소화하는 함수와 일치)
      * robust한 예측시 -> median사용
      * 목적에 따라 estimator, 추정량이 달라짐 
 
        <img width="300" alt="image" src="https://user-images.githubusercontent.com/93971443/191896352-76ed5b49-d2fb-439b-ab9e-4f0d2a5b3f23.png">

> ### 기대값
> * 데이터를 대표하는 통계값, 다른 통계적 범함수 계산에도 사용됨(확률분포종류에 따라 적분, sum 달라짐)
> <img width="702" alt="image" src="https://user-images.githubusercontent.com/93971443/191896929-42864462-f90c-4a36-8850-27913f5cc2ca.png">
>
> * 분산, 첨도, 공분산 등 여러 통계량 계산(통계적 범함수 계산)
> <img width="500" alt="image" src="https://user-images.githubusercontent.com/93971443/191896994-1cfaff54-579a-4494-a770-a474ead59fef.png">

## 몬테카를로 샘플링
* 명확한 확률분포 모르고 sampling 방법만 알 때 사용(기계학습에서 자주 사용)
* sampling 데이터를 넣고 산술평균계산
<img width="600" alt="image" src="https://user-images.githubusercontent.com/93971443/191897284-12e46492-15df-4f16-bc30-f51c145fd12b.png">

* 연속형/이산형 상관없이 성립
* 단, 샘플링시 독립추출(independent and identically distribution : i.i.d)을 보장해야한다.
* 만족시 대수의 법칙에 의해 수렴성 보장


## 딥러닝에서의 확률론
* 손실 함수의 작동 원리는 데이터 공간을 통계적으로 해석 후 유도
* 회귀 / 분류 모델에서 분산 / 불확실성을 '최소화'하는 방향으로 학습을 유도
* Data(분포)와 예측(분포)이 틀릴 확률을 최소화 
