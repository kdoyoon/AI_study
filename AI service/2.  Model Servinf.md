# Model Serving

* * *

### Serving
- Production(Real World) 환경에 모델을 사용할 수 있도록 배포
- 머신러닝 모델을 개발하고, 현실 세계(앱, 웹)에서 사용할 수 있게 만드는 행위
- 서비스화라고 표현할 수도 있음
- 머신러닝 모델을 회사 서비스의 기능 중 하나로 활용
- online / batch serving이 대표적
- 클라이언트(모바일 기기, IoT Device 등)에서 Edge Serving도 존재

* * *

### online serving
##### Web Server Basic
- HTTP를 통해 웹 브라우저에서 요청하는 HTML 문서나 오브젝트를 전송해주는 서비스 프로그램
- 요청(request)를 받으면 요청한 내용을 보내주는(response) 프로그램
- web server(직원)은 client(손님)의 다양한 요청을 처리해줌
- 모든 웹 서버는 request와 response로 나뉨
- Machine Learning Server는 Client의 다양한 요청을 처리해주는 역할(데이터 전처리, 모델을 기반으로 예측 등)
  - 예시
    - 크롬에서 유튜브 접속 시
    - HTTP requeset -> 크롬(Browser, Client) / www.youtube.com(Server) -> HTTP response
    - 회원가입시
    - 회원가입 요청(request) <-> 검증 후 응답(response)

- 머신러닝 모델 서버
- 어떤 데이터(input)를 제공하며 예측해달라고 요청(request)하면, 모델을 사용해 예측 값을 반환(response)하는 서버

<img width="767" alt="image" src="https://user-images.githubusercontent.com/93971443/200275438-3dc8f9e0-e4b5-45e2-9ddc-17012cc7092e.png">

##### API
- 운영체제나 프로그래밍 언어가 제공하는 기능을 제어할 수 있게 만든 인터페이스
- 사용자들이 프로그램이나 어플리케이션을 사용할 수 있도록 여러 기능이 존재
- 특정 서비스에서 해당 기능을 사용할 수 있도록 외부에 노출 : 기상청 API, 지도 API
- 라이브러리의 함수 : Pandas, Tensorflow, PyTorch


##### online serving basic
<img width="623" alt="image" src="https://user-images.githubusercontent.com/93971443/200276689-9bc9f7ae-7705-439a-b903-0852d2d4e9d6.png">

- 요청(request)이 들어올 때 마다 실시간으로 예측
- 클라이언트(애플리케이션)에서 ML 모델 서버에 HTTP 요청(request)하고, 머신러닝 모델 서버에서 예측한 후, 예측 값(응답)을 반환(response)
- ML 모델 서버에 요청할 때 필요시 ML 모델 서버에서 데이터 전처리를 해야할 수 있음
- 서비스의 서버에 ML서버를 포함하는 경우도 있고, ML서버를 별도로 운영하는 경우도 존재
- 회사에서 개발 조직과 데이터 조직의 협업하는 방식에 따라 다르게 개발할 수 있음   

- online serving을 구현하는 방식
  - 직접 API 웹 서버 개발 : FLASK, FastAPI 등을 사용해 서버 구축
  - 클라우드 서비스 활용 : AWS의 SageMaker, GCP의 Vertex AI 등
    - 장점
    - 직접 구축해야하는 MLOps의 다양한 부분이 만들어짐
    - 사용자 관점에선 PyTorch 사용하듯 학습 코드만 제공하면 API서버가 만들어짐
    - 아쉬운 점
    - 클라우드 서버가 익숙해야 잘 활용할 수 있음
    - 비용 문제 : 직접 만드는 것보단 더 많은 비용이 나갈 수 있음
  - Serving 라이브러리 활용 : Tensorflow Serving, Torch Serve, MLFlow, BentoML 등

- online serving에서 고려할 부분
1. Input 데이터를 기반으로 database에 있는 데이터를 추출해서 모델 예측해야하는 경우
  - 데이터는 다양한 공간에 저장되어 있을 수 있음
  - 데이터를 추출하기 위해 쿼리를 실행ㅇ하고, 결과를 받는 시간이 소요
2. 모델이 수행하는 연산
  - RNN. LSTM 등은 회귀 분석보다 많은 연산을 요구하고, 더 오래 걸림
  - 이를 위해 모델을 경랴와하는 작어비 필요할 수 있으며, 복잡한 모델보다 간단한 모델을 가용하는 경우도 존재
3. 결과 값에 대한 보정이 필요한 경우
  - 머신러닝 알고리즘에서 유효하지 않은 예측값이 반환될 수 있음
  - 예를 들어 집 값을 예측하는데, 0 이하의 마이너스 값이 나올 수 없음
  - 이런 경우 결과를 보정하는 코드가 필요할 수 있음(ex: 집 값이 마이너스가 나오면 0으로 표시함)

* * * 

### Batch serving
<img width="585" alt="image" src="https://user-images.githubusercontent.com/93971443/200280822-348f3102-3284-4ae3-a7cc-8953d6f69c95.png">


- workflow scheduler
  - 작업을 특정 기간 단위 등으로 실행
- 주기적으로 학습을 하거나 예측을 하는 경우
- 한번에 많은 예측을 실행
- 함수 단위를 '주기적'으로 실행함
- 장점
  - 실시간이 필요없는 대부분의 방식에서 활용가능
  - online serving보다 구현이 간단함, latency가 문제가 되지 않음
- 단점
  - 실시간으로 활용할 수 없음
  - cold start 문제 : 오늘 새로 생긴 컨텐츠는 추천할 수 없음
 
 
 * * *
 
 ### online Serving vs. Batch serving
 
 - Input 관점
 
 ||online|Batch|
 |:---:|:--:|:--:|
 |Input 관점| 데이터 하나씩 요청하는 경우 | 여러가지 데이터가 한꺼번에 처리되는 경우|
 |output 관점|API 형태로 바로 결과를 반환해야 하는 경우, 서버와 통신이 필요한 경우| 1시간에 1번씩 예측해도 괜찮은 경우|
 ||처음부터 API 형태로 만들 필요X, 실시간 모델 결과가 어떻게 활용되는지에 대한 생각 필요 | 결과 Database에 저장|
 
 ### 우선 Batch serving으로 모델을 운영하면서 점점 API 형태로 변환
 

### ※ 위 내용 및 이미지는 네이버 부스트캠프 교육 자료를 참고하였습니다.





