# CNN architectures

* * *

### AlexNet

- 첫 성공적 CNN
- 구조

<img width="700" alt="image" src="https://user-images.githubusercontent.com/93971443/195969264-e06873d1-d249-4dc7-8922-4e74f32a2891.png">

- 당시 GPU 부족으로 인한 cross가 일어나는 구조
- tensor 형태인 output을 vector화 해줘야함 -> flatten 해줌
- architecture
  - input size : 227 X 227
  - 5 Conv layer + 3 FC layer
- Receptive field
  - 한 픽셀(점)을 도출해주는데 영향을 준 입력 size 영역(Input영역까지 내려가 줘야함
<img width="450" alt="image" src="https://user-images.githubusercontent.com/93971443/195969463-876bfb3d-a028-477a-ae67-2fde13d15460.png">


* * *

### VGGNet
<img width="350" alt="image" src="https://user-images.githubusercontent.com/93971443/195969603-752b7b27-e3aa-46a0-8b11-e10da4d0774e.png">

- 특징
  - Deeper architecture
  - Simpler architecture
  - Better performance
  - Better generalization

- architecture
  - Input size : 224 X 224
  - 3X3 conv filter + 2X2 max pooling
    - 3X3 conv 사용 이유 : receptive field를 크게 유지하기 위함
  - ReLU 사용

* * *
#### deep and wide NN
- Larger receeptive field & more capacity and non-linearity
- 위의 이유로 층이 더 많아질수록 학습을 많이 할 수 있음
- 그러나 gradient vanishing / explosion, computational complex, degradation 문제 존재 -> 학습이 항상 잘되는 것이 아님

* * *
### GoogLeNet

- Inception model 사용
  - 여러 개의 conv filter 사용
  - 각 filter의 output들을 채널축으로 concat
  - network size가 커질 수록 computational resource도 증가 -> 1X1 conv사용
  - ※ 1X1 conv : 채널의 수를 줄여줌
 <img width="729" alt="image" src="https://user-images.githubusercontent.com/93971443/195971809-2374d5e1-d878-4526-bb6a-b2510a51da5d.png">

- architecture
  - stem network
  - stacked inception modules
  - Auxiliary classifier
    - gradient vanishing 문제를 해결하기 위해 중간 중간 loss를 전달함
    - training 동안에만 사용 test시엔 사용X
  - Classifier output(single FC layer
<img width="731" alt="image" src="https://user-images.githubusercontent.com/93971443/195971989-adbde77e-cd85-4526-9b08-d1732314b9b5.png">


* * *

### ResNet
- optimization애 의한 degradation problem
- plain layer : $H(X)$를 바로 학습하기 어렵다고 가정

<img width="662" alt="image" src="https://user-images.githubusercontent.com/93971443/195972218-b6dbdc7a-4a89-43bf-9b99-8df8817902d6.png">

- 단순히 쌓는 plain layer대신 shortcut을 추가한 residual block을 사용 -> gradient vanishing 해결
<img width="766" alt="image" src="https://user-images.githubusercontent.com/93971443/195972357-8bbf9ee0-6bd9-4597-9254-1643a9d4a98b.png">

* * *

## 정리
- AlexNet
  - simple CNN architecture
  - simple computation, but heavy memory size
  - Low accuacy

- VGGNet
  - simple with 3X3 convolutions
  - Highest memory, the heaviest computation

- GoogLeNet
  - inception modeul and auxiliary classifier

- ResNet
  - deeper layers with residual blocks
  - Moderate efficiency(depending on the model)



### ※ 위 내용 및 이미지는 네이버 부스트캠프 교육 자료를 참고하였습니다.
