# Conditional Geraerative Model

* * *

* 조건에 대해 X가 나올 확률을 구하고 이미지를 생성함
* 확률분포를 모델링
* 다양한 task에 적용가능
  - audio super resolution(저품질 -> 고품질)
  - machine translation
  - article generation with the title

#### Generative model Vs. conditional generative model
- generative model : 랜덤한 이미지 생성만 함, 조작은 불가능
- conditional generative model : user의 의도를 반영한 이미지 생성 가능
<img width="776" alt="image" src="https://user-images.githubusercontent.com/93971443/196583600-65b4647a-958d-4eb6-a942-8ac95640bbbf.png">


#### Recap
- Generative Adversarial Network
  - Generator은 Discriminator가 가짜 데이터를 진짜 데이터로 착각하게 만들려고 학습함
  - Discriminator는 진짜/가짜를 잘 판별할 수 있도록 학습함
  - 한쪽 성능이 좋아지면 그에 상응하여 더 잘 속이거나 판단할 수 있도록 학습하게 됨(상호작용_적대적 학습법)
  <img width="750" alt="image" src="https://user-images.githubusercontent.com/93971443/196583919-879916cb-0c2c-4944-a3ba-93a7ac45da7a.png">
  
- Basic GAN Vs. Conditional GAN 구조

  <img width="664" alt="image" src="https://user-images.githubusercontent.com/93971443/196584060-699313bc-ad8d-4f54-9094-0924aca6c92f.png">
  
  - 큰 차이 없이 INPUT만 추가한 구조
  - conditional GAN도 Adversarial learning의 장점을 그대로 사용 가능

 #### 예시
- Image to Image translation
  - 다른 스타일의 이미지로 변경할 수 있다.
  <img width="593" alt="image" src="https://user-images.githubusercontent.com/93971443/196584704-05c94d66-f0c8-4168-bf4f-96fd831545f6.png">

- Super Resolution
  - 저해상도의 이미지를 고해상도로 변경
  - 기본 회귀 모델과 Super Resolution model의 차이
    <img width="691" alt="image" src="https://user-images.githubusercontent.com/93971443/196585040-774e1776-48b6-4e2d-a328-f3115beedc0e.png">
    
     - 기본 회귀 모델은 평균을 사용하기 때문에 실제 이미지들과 어느 정도 거리를 유지함->해상도는 높지만 좋지 않은 이미지
     - GAN은 Discriminator의 판별을 피하기 위해서 실제 데이터와 가깝게 생성
     
    <img width="519" alt="image" src="https://user-images.githubusercontent.com/93971443/196585306-b1912c24-d4dc-488c-bc80-042e6442a8c2.png">
    
    <img width="643" alt="image" src="https://user-images.githubusercontent.com/93971443/196585457-4a418045-1920-4c9f-9b46-c79afa5c11a0.png">
    
    - l1 loss 사용시엔 black/white가 아닌 gray를 생성해 Discriminator에 걸리지만
    - GAN loss 사용시 target에 최대한 가깝게 생성

* * *
## Image traslation GANs

### Pix2Pix
* translation an image to a corresponding image in another domain(e.g., style)
* L1 loss와 GAN loss 둘 다 사용
* GAN loss만 사용하는 경우에 pair간 독립적으로 계산 즉, y와 비슷한 이미지를 생성할 수 없음(blur image 생성)
* 당시 GAN loss 계산의 불안정함이 있었기 때문에 l1 loss가 guide 역할
* L1 loss를 통해 y와 비슷한 이미지를 생성하고 GAN loss를 통해 이미지를 realistic하게 표현

<img width="679" alt="image" src="https://user-images.githubusercontent.com/93971443/196586528-b1fa913c-03b1-4f11-addc-c447e5aecaa9.png">


### CycleGAN
* Pix2Pix 모델은 pairwise 데이터(->supervised)를 필요로하는데 구하기가 어려움
* paired data를 사용하지 않고 사진의 스타일 변경 가능

<img width="790" alt="image" src="https://user-images.githubusercontent.com/93971443/196595613-5df030ce-d7a2-4828-9c19-eccce67d765c.png">

- GAN loss : X->Y / Y->X 생성시 비슷하게 생성하였는지 확인(양방향 학습), GAN만 사용시 model Collapse 발생 가능(다양한 input 하나의 output)

<img width="586" alt="image" src="https://user-images.githubusercontent.com/93971443/196596375-1061222f-68ef-4fb7-95bf-1084c798d23f.png">

- Cycle loss : X->Y / Y->X 다시 돌아올 때 원본이 유지되도록 함(self-supervised로 볼 수도 있음)

<img width="645" alt="image" src="https://user-images.githubusercontent.com/93971443/196596427-1828cb5c-2b1d-4aa3-9152-c1899c2cef81.png">


### Perceptual loss
* supervised learning이 가능할 땐 Cycle GAN보다 GAN을 사용하는데 alterning training이 요구되기 때문에 학습시키기 어려움
* GAN loss Vs. Perceptual loss
  - GAN(=adversarial loss) : 상대적으로 학습시키기 어렵지만 pre-trained된 네트워크가 필요없음, 따라서 다양한 응용 가능
  - Perceptual : 학습시키기 쉬움, pre-trained된 네트워크 필요

* Perceptual loss란
  - 인간의 시각 인식 체계와 유사
  - pre-trained된 네트워크 사용시 이미지를 인식공간으로 볼 수 있음

<img width="643" alt="image" src="https://user-images.githubusercontent.com/93971443/196597784-97aaf8e6-7c62-4daa-8d51-d025c1dba193.png">

- Image Transform Net : input 이미지를 하나의 style로 변경하여 출력(data에 따라 style 바뀜)
- Loss Network : 생성된 이미지와 타켓 이미지 사이 style, feature loss를 계산함   
                - 보통 pre-trained된 VGG net을 사용(VGG가 feature을 추출)   
                - VGG는 학습하지 않고(fixed) Image transform net만 학습을 함

- Reconstruction loss
  * Feature reconstruction loss

  <img width="752" alt="image" src="https://user-images.githubusercontent.com/93971443/196604774-5fc9eb31-b0fb-4097-947f-e9848e28fcad.png">
  
  - Transformed 된 이미지가 원래 이미지 모양을 유지하도록 만들어줌 
  - content target엔 원래 X를 넣어줌
  - Transformed 된 이미지와 원래 이미지 사이의 L2-loss를 계산

  * Style reconstruction loss

  <img width="785" alt="image" src="https://user-images.githubusercontent.com/93971443/196604791-cbda677a-3f62-4361-90e1-a01ee1224da9.png">
  
  - Gram matrix 사용 
  - 각각 loss network에 넣어 feature 추출 후 concat -> tensor형태의 feature map 나옴(channel : 하나의 특징을 나타냄)
   
  <img width="674" alt="image" src="https://user-images.githubusercontent.com/93971443/196606368-7021771c-d964-4c19-a775-2e8730a36bf0.png">

  - gram matrix : 가로축 채널, 세로축 채널이 공통적으로 발견되는지 그 연관성 정도를 나타내는 matrix



### ※ 위 내용 및 이미지는 네이버 부스트캠프 교육 자료를 참고하였습니다.
