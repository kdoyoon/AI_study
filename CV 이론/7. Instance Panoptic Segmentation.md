# Instance Panoptic Segmentation

* * * 

### Instance Segmetation

* Semantic segmentation + ditinguishing instances
* Mask R-CNN
  - 정수 좌표로 부터만 feature을 추출하는 ROI pooling 대신 interpolation을 사용하여 소수점까지 정교한 연산이 가능한 ROI align 사용
  -  Faster R-CNN + Mask branch
<img width="819" alt="image" src="https://user-images.githubusercontent.com/93971443/196388532-5c7145db-b9a0-4a2f-925b-19a9cefde911.png">

    - Mask branch : upsampling을 통해 채널수를 줄임, class 별로 mask 생성 후 binary prediction 진행

* YOLACT(You Only Look At CoefficienTs)
  - real time semantic segmentation
  <img width="748" alt="image" src="https://user-images.githubusercontent.com/93971443/196390213-66091224-c814-4634-88ec-f1d8ecc20820.png">
  
  - FPN을 이용하여 고해상도의 이미지를 뽑아냄
  - Mask R-CNN에서 class마다 mask를 생성한 것과 달리 추후에 mask로 합성될 수 있는 재료(soft segmentation component) prototypes 생성
  - 생성된 prototypes들과 prediction head에서 뽑아낸 mask coef와 선형 결합을 통해 mask response map을 생성

* YolactEdge
  - Yolact를 비디오에 적용한 모델
<img width="784" alt="image" src="https://user-images.githubusercontent.com/93971443/196391640-2ee94e8e-4343-4581-b992-65b504631215.png">
  
    - previous keyframe에서 key frame feature 전달
    - 소형화된 edge device에서도 빠르게 동작할 수 있도록 성능은 유지한채로 간소화

* * *

### Panoptic segmentation
* Stuff + Instances of Things
* 배경까지 고려
* UPSNet
  - FPS로 feature extraction
  - Semantic & Instance Head에서 나온 결과를 Panoptic Head에서 융합
    - Semantic : fully conv layer, Semantic prediction 결과 
    - Instance : 물체 detection, box regression, mask 추출
  <img width="751" alt="image" src="https://user-images.githubusercontent.com/93971443/196394219-5cc0fe2e-a684-427b-abda-b82a5e03c39f.png">
  
    - instance head에서 나온 결과를 semantic thing 부분과 합쳐 기존 이미지 사이즈와 맞춰줌
    - 배경에 해당하는 stuff 부분은 바로 panoptic layer로 감
    - thing 부분에다 instance 부분을 제거하고 배경에 대한 정보만 넘김
  <img width="757" alt="image" src="https://user-images.githubusercontent.com/93971443/196395173-5b586db8-cccb-4ab4-91fa-ae1ebc1119e4.png">

* VPSNet(for video)
  1. Align reference features onto the target feature map(Fusion at pixel level)
      - 영상이 움직일 때 움직이는 대응점들을 특정하여 motion map을 만들어 warping 진행 후 feature map에 concat시킴
      - 현재 이미지에서 파악할 수 없었던 것들이 이전, 이후 영상을 통해 추론할 수 있게됨
      - 높은 detection 성능 및 여러 frame feature 사용으로 시간 연속성을 가짐

  2. Track module associates different different object instances(Track at instance level)
      - 기존 ROI와 현재 ROI의 연관성을 Track head를 통해 찾아줌(어떤 id를 가진 물체였는지 파악 가능)

  3. Fused-and-tracked modules are trained to synergize each other
      - BBox head, Mask head, Semantic head를 panoptic map으로 합쳐줌

<img width="755" alt="image" src="https://user-images.githubusercontent.com/93971443/196398373-9e5b165a-190b-46e9-bfbd-a81e16454872.png">

* * * 

### Landmark localization
* keypoint estimation : predicting the coordinates of keypoints
* coordinate regression Vs. heatmap classification
  - coordinate regression : usually inaccurate and biased
  - heatmap classification : better performance but high computational cost(픽셀 하나씩 모두 계산하기 때문)

* Hourglass network
  - hourglass가 stacked 된 구조
  - U-net과 유사(skip connection이 존재)
  <img width="463" alt="image" src="https://user-images.githubusercontent.com/93971443/196400426-ca1619a2-4862-4a5a-8599-93f641c2cb1c.png">
  
    - 바로 concat되는 u-net과는 달리 하나의 CNN을 거쳐 더해지게 됨(dimension이 변경되지 않음)
