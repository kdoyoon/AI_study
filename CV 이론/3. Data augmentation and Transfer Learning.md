# Data Augmentation

* * *
### augmentaion 사용 이유
- 학습은 데이터 셋을 통해 진행됨
- 데이터 셋은 항상 편향되어 있음(real data와 다름)
- 데이터 셋은 real data의 극히 일부만 갖고 있음
- real data distribution을 다 표현하지 못한다는 문제를 갖게됨

<img width="520" alt="image" src="https://user-images.githubusercontent.com/93971443/195970004-d2a555bb-a7b8-4531-82d2-902cb1b00fde.png">

- augmented된 data를 통해 빈 공간들을 채워 학습을 진행함

* * *

### Data augmentation
- 목표 : 훈련 데이터셋의 distribution을 real data distribution과 유사하게 만듦
- 종류
  - Brightness adjustment
  - Rotate, flip
  - Crop
  - Affine transformation
    - line, length ratio, parallelism 유지한 변형
  - CutMix(= 합성)
    - label도 합성된 비율만큼 합쳐줘야함
  - RandAugment
    - 앞서 언급한 augmentation들을 자동적으로 최적의 augmentation 순서를 찾아줌
    - 변형의 정도를 결정해줘야함(강하게 vs 약하게)
    - 보통 좋다는 연구 결과가 있음

* * *

# Transfer Learning(전이학습)

- 큰 사이즈의 데이터로 학습된 모델을 가져와 우리가 가지고 있는 작은 사이즈의 데이터에 적용
- 한 데이터 셋에서 학습된 지식은 다른 데이터 셋에서도 적용 가능하다에서 착안
- pretrained model의 fc layer을 잘라내고 우리 데이터에 맞는 새로운 fc layer을 붙임
<img width="750" alt="image" src="https://user-images.githubusercontent.com/93971443/195970346-cefa2f44-2e55-40bb-a2cf-be4d07e534ea.png">

- 빨리 우리 데이터에 적용할 수 있도록 모델의 학습률을 다르게 설정(fine tuning)
<img width="400" alt="image" src="https://user-images.githubusercontent.com/93971443/195970394-a70df40b-6c23-4c43-a06d-cffd569d4ebf.png">

* * *

### Knowledge Distillation
- (큰) 모델이 학습한 것을 다른 작은 모델로 넘기는 것
- model compression을 위해 사용
- pseudo labeling을 위해 사용
- Teacher-Student network Structure
  - student network가 teacher network를 모방
  - Unsupervised learning

<img width="591" alt="image" src="https://user-images.githubusercontent.com/93971443/195970659-cb6bec88-29e9-4b48-82a9-62f8105e2e58.png">   
   
※ back propagation은 student model쪽으로 진행

<img width="700" alt="image" src="https://user-images.githubusercontent.com/93971443/195970753-b9e4e055-9c8d-4edf-ab49-0d0921b90252.png">

- Distillation loss
  - KL div
  - Loss = difference between the teacher and student network's inference
  - learn what teacher network knows by mimicking   
  - 
- Student loss
  - Cross Entrophy
  - Loss = difference between the student network's inference and true label
  - learn the "right answer"

<img width="643" alt="image" src="https://user-images.githubusercontent.com/93971443/195970957-28f86fc2-bbbe-430c-8e9e-53a5aec22a04.png">


* * *

### Semi-Supervised learning
- pseudo-labeling unlabeled data using a pre-trained model, then use for training
<img width="575" alt="image" src="https://user-images.githubusercontent.com/93971443/195971046-da7e5185-a47e-473b-af12-7be698ba0d65.png">

#### Self Training
<img width="740" alt="image" src="https://user-images.githubusercontent.com/93971443/195971105-a5c4332c-2d44-4876-8b7f-4ba5a1e22a80.png">

- 위 과정을 반복하여 학습을 진행
<img width="728" alt="image" src="https://user-images.githubusercontent.com/93971443/195971124-0ab3b638-5e87-40ec-98b0-fcb77716d5cc.png">



### ※ 위 내용 및 이미지는 네이버 부스트캠프 교육 자료를 참고하였습니다.
