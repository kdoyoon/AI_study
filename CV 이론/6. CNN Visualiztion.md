# CNN Visualization

* * * 

- CNN 내부는 학습된 weighted들의 조합으로 이루어져 있어 파악하기 힘듦(black box)
- 따라서 visualization이 중요

* * *

### Visualization의 종류
- Analysis of model behaviors(모델의 특성 분석)
  - Parameter examination(filter visualization, factorization lens)
  - Feature analysis(t-SNE, Gradient ascent)
- Model decision explanation(결론 출력 분석)
  - Sensitivity analysis(Saliency map, GradCAM)
  - Decomposition(DeepLIFT, LPR)
<img width="750" alt="image" src="https://user-images.githubusercontent.com/93971443/196133418-b1a20c77-6e7a-446a-b364-1904f7bb4b24.png">

* * *

### Analysis of model behaviors
<img width="768" alt="image" src="https://user-images.githubusercontent.com/93971443/196127638-bbf3f9b4-36a4-4871-ba12-9171e13a3278.png">

- Embedding feature analysis
  - Nearest Neighbors in feature space
    - HIGH 부분의 과정
    - search the nearest neighbors of features from DB
    - DB에 미리 준비되어있는 train 데이터의 feature를 추출하여 저장함
    - 새롭게 들어오는 Query 이미지에 대해 검색된 주변 이미지들로 분석함
    - 즉, query image와 모델이 찾아낸 neighbor image간 픽셀별로 distance를 계산하여 비슷한 이미지인지 판별
      - 이미지의 위치가 다르거나 포즈가 다른 이미지 들은 올바르게 판단할 수 없는 경우가 많음(픽셀별 비교)
      - 만약 포즈가 달라도 명확하게 neighbor로 구분시엔 위치변화에 강인하게 컨셉을 제대로 학습했다는 의미
<img width="704" alt="image" src="https://user-images.githubusercontent.com/93971443/196130595-6b1b4d67-74de-42e1-92da-af12ef2485b0.png">

※ 빨간 부분: query image 미리 표현   

  - Dimentionality Reduction
    - NN in feature space -> 차원이 너무 높아 인간이 이해하기 어렵다는 단점이 있음
    - 차원을 줄여 인간이 이해할 수 있도록 함
    - t-SNE : 2차원 공간상으로 나타냄
 
<img width="517" alt="image" src="https://user-images.githubusercontent.com/93971443/196129755-03de2adb-0c2c-43a3-975e-129c4d5df675.png">

- Activation investigation
  - Layer activation
    - Behaviors of mid to high-level hidden units
    - 특정 hidden node를 선택하여 출력함
    - 선택한 hidden node의 역할을 파악할 수 있음
    - hidden node가 찾은 특징을 파악(ex. 얼굴, 난간)

<img width="558" alt="image" src="https://user-images.githubusercontent.com/93971443/196130997-3f91179b-6b0a-4c28-889d-cfe26737957e.png">

  - Maximally activation patches
    - mid-level 분석에 용이
    - Sequenece
      1. Pick a channel in a certain layer
      2. Feed a chunk of images and record each activation value(of the chosen channel)
      3. Crop image patches around maximum activation values
    - score가 가장 높은 곳의 위치를 파악하고 receptive field에서 해당 부분을 뜯어옴
   
- Class Visualization
  - 예제 데이터를 사용하지 않고 네트워크가 기억하고 있는 이미지를 시각화하여 판단
  <img width="297" alt="image" src="https://user-images.githubusercontent.com/93971443/196132214-17df587b-c9f5-4d1b-a953-553db924ff05.png">
  
  - 위 사진은 bird, dog 클래스에 대한 네트워크 예상치 확인한 결과
  - target class외에도 주변 부분(배경, 사람)도 나온 것을 볼 수 있음(->bias됨을 의심할 필요있음)
  - 이 방법은 최적화를 통해 진행
  
  <img width="456" alt="image" src="https://user-images.githubusercontent.com/93971443/196136336-f4a503bf-a256-40ac-a178-f8f06d59c516.png">
  
  - 위와 같이 2개의 loss function을 합쳐서 씀
  - 추상적인 값이 매우 클 경우 우리가 알고 있는 또는 이해할 수 있는 영상으로 유도하기 위해 사용
   
  - Gradient ascent
    - Sequenece
     1. Get a prediction score(of the target class) of a dummy image(blank or random initial)
     2. Backpropagate the gradient maximizing the target class score w.r.t the input image
     3. Update the current image

* * *

### Model decision explanation

- Saliency test
  * Occlusion map
    - 일정 부분을 가리고 예측을 진행
    <img width="780" alt="image" src="https://user-images.githubusercontent.com/93971443/196138223-17baedfa-98ed-4297-9b59-9330df5e51d2.png">
    - prediction score가 급격히 떨어진 부분은 중요한 부분임을 고려하여 heatmap으로 나타냄

  * via Backpropagation
    - Analysis of model behaviors의 gradient ascent와 유사
    - 그러나 특정 이미지를 넣어 classification에 큰 영향을 끼친 부분을 heatmap으로 표시
    - Sequence
      1. Get a class score of the target source image
      2. Backpropagate the gradient of the class score w.r.t input domain
      3. Visualize the obtained gradient magnitude map(optionally, can be accumulated)
  
   <img width="703" alt="image" src="https://user-images.githubusercontent.com/93971443/196139393-35d6d58e-50f1-467b-9880-8bfb410c7a40.png">
   
   
   
- Backpropagate features
  * Rectified unit(backward pass)
    - (일반적 CNN)forward 연산 시 0이하였던 unit들을 backpropagation시에도 마스킹 처리
    - deconvnet은 backpropagation 시점 기준으로 0이하인 값들에 대해 마스킹 처리
    - 역방향 ReLU 적용과 같은 효과
    - 좀 더 휴리스틱한 결과 얻을 수 있음
   <img width="756" alt="image" src="https://user-images.githubusercontent.com/93971443/196141435-e2e1ee7a-490a-4dff-8758-77d231b64466.png">
   
   
  * Guided backpropagation
    - backward시 forward시점과 현재 시점 0이하인 값들 다 마스킹 처리 
    <img width="568" alt="image" src="https://user-images.githubusercontent.com/93971443/196142618-746826b1-db00-4cc0-b071-b22d5cd65103.png">

    - forward & backward 연산시 좋은 영향만 살아남게 됨(결과 good)


- Class activation mapping
  * Class activation mapping(CAM)
    - 어떤 부분이 마지막 결정에 큰 영향을 줬는지 시각화해줌
    <img width="639" alt="image" src="https://user-images.githubusercontent.com/93971443/196143087-a54d9516-302d-41f5-99c2-975b1869ecbb.png">

    - CNN 모델에서 마지막 FC layer 대신 Glabal average pooling layer을 적용
    - 후에 단 하나의 FC layer을 통과 시킴
    <img width="669" alt="image" src="https://user-images.githubusercontent.com/93971443/196143731-7fd1b518-d2df-4185-b7f2-5202807ec8fa.png">

    - 모든 연산이 선형 연산이므로 위와 같이 순서를 바꾸어줄 수 있음
    - $CAM_c(x,y)$ 는 GAP 적용전이므로 아직 공간정보를 가지고 있음->시각화시에 heatmap 형태로 나옴
    - weakly supervised learning이라 부르기도 함

  <img width="598" alt="image" src="https://user-images.githubusercontent.com/93971443/196144881-abf1c5ab-8fb9-4f01-8bdb-1072ec283ed5.png">
  
    - 단점 : 모델을 구조 수정 및 재학습이 필요 -> 모델 성능이 바뀌게 됨

  * Grad-CAM
    - 모델을 변경하지 않고 CAM을 뽑아내는 방식
    - 국한된 task없이 backbone만 CNN이면 사용 가능 
    - importance weight를 구하는 것이 관건

     <img width="735" alt="image" src="https://user-images.githubusercontent.com/93971443/196146145-d325482d-9659-4d1d-aca9-2351793f8774.png">


    - 원하는 activation map까지만 backpropagation 진행
    - 클래스 c에 대한 loss 구함 
    - 위 과정을 통해 구한 weight이 alpha가 됨
    - 선형결합하여 ReLU 적용
    - 위 결과를 시각화, heatmap이 나옴
    - 다양한 task에 적용 가능
    <img width="727" alt="image" src="https://user-images.githubusercontent.com/93971443/196146957-a4a8e862-5526-435b-a348-10bedff89294.png">


### ※ 위 내용 및 이미지는 네이버 부스트캠프 교육 자료를 참고하였습니다.
