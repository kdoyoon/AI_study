# Optimization

* * *

### Generalization
- 학습된 모델이 처음 보는 데이터(test data)에 대해 학습할 때의 예측과 얼마나 비슷하게 예측하는지 확인

<img width="650" alt="image" src="https://user-images.githubusercontent.com/93971443/193747866-9b7cae17-cde5-49db-a70c-9a252259533a.png">

- Generalization gap이 작을 수롣 generalization performance가 좋음
- 그러나 모델 학습 과정에서 모델 자체가 학습 제대로 안됐을 경우에도 performance가 좋은 경우가 있음(유의)


### Overfitting / Underfitting
- overfitting(과적합) : 학습 데이터에만 과도하게 학습된 상태, 다른 데이터에 대해선 X
- underfitting(과소적합) : 학습 데이터에도 제대로 학습하지 못한 상태

<img width="700" alt="image" src="https://user-images.githubusercontent.com/93971443/193748645-2cc88d21-8b5f-460a-96b5-2b948c6891c4.png">


### Cross Validation

- test set을 제외한 데이터 셋을 나눠 교차 검증을 하는 방식(test set은 학습시 절대 사용하면 안됨)
- train / validation set으로 나눠 학습을 진행한 후 결과들의 평균을 냄
- validation set은 hyper parameter tuning에 사용됨

<img width="550" alt="image" src="https://user-images.githubusercontent.com/93971443/193749294-083fef6d-5565-4a65-88ec-383b38000b4a.png">


### Bias and Variance

<img width="459" alt="image" src="https://user-images.githubusercontent.com/93971443/193749622-377320e9-bebf-4d6e-ac4a-ba20686ecea6.png">

- low bias, low variance가 가장 이상적, 그러나 어려움

##### Bias and Variance trade-off
- noise가 껴있는 모델에 대해서 bias와 variance를 둘 다 줄이기는 불가능에 가까움
- bias가 높아지면 variance가 작아지고, bias가 작아지면 variance가 높아지는 trade-off 관계를 갖음


### Bootstrapping

- 전체 모델의 불확실성을 측정하기 위해 사용
- sub sampling을 통해 여러 개의 data sample을 만드는 것

#### Bagging
- bootstrapping을 이용하여 여러 모델을 학습 시키는 것

#### Boosting
- 여러 개의 weak learner들을 결합하여 순차적으로 이전 모델의 오차들을 학습 기키는 것

* * *

### Gradient Descent Methods

##### batch size matters

  - computed from single sample : SGD
  - computed from subset of data : Mini-batch gradient descent
  - computed from whole data : Batch gradient descent


  - Large size vs. Small size

  <img width="597" alt="image" src="https://user-images.githubusercontent.com/93971443/193752003-c7b8fb65-05b4-4c70-af40-79ddbca897c4.png">

  - large size : sharp minimizer로 converge
  - small size : flat minimizer로 converge
  - 일반적으로 flat minimizer의 generalization performance가 좋음 -> 적절한 사이즈가 중요   

#### 종류
- SGD
- Momentum
  - 이전 gradient의 방향도 반영하기 때문에 데이터를 두번 보는 효과가 있어 성능이 좋음
- Nestrov accelerated gradient
- Adagrad
- Adadelta
- RMSprop
- Adam
  - Adaptive Moment Estimation
  - momentum과 adaptive learning rate를 효율적으로 결합함
  - 가장 많이 쓰이는 모델
 
* * *

### Regularization

- Early Stopping
- Parameter Norm penalty
- Data augmentation
  - data의 label이 변하지 않는 선에서 진행
- Noise robustness
- Label Smoothing
- Dropout
- Batch Normalization




### ※ 위 내용 및 이미지는 네이버 부스트캠프 교육 자료를 참고하였습니다.
