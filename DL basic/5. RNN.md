# RNN

* * *

### Sequential Model
* 음성, 비디오 등 연속적인 데이터
* Input의 차원을 알 수 없음

* Naive model

<img width="550" alt="image" src="https://user-images.githubusercontent.com/93971443/194322815-5a171d8d-6842-4d69-8d89-868540ddbb7a.png">

* Autoregressive Model
  - 정해진 기간의 과거만 반영
  - 예시) AR1 : 과거 1개 반영
<img width="550" alt="image" src="https://user-images.githubusercontent.com/93971443/194322961-41eacae1-7a57-499b-81cd-1c0d553ef5e7.png">

* Markov Model
  - 현재는 바로 전 과거에만 dependent
  - 상당히 많은 정보를 버리게 됨
<img width="525" alt="image" src="https://user-images.githubusercontent.com/93971443/194323267-ba54b450-1638-4612-8712-32d855d0a87e.png">

* Latent autoregressive Model
  - 중간에 hidden state를 추가함
  - hidden state는 과거 정보를 summarize

<img width="592" alt="image" src="https://user-images.githubusercontent.com/93971443/194324238-13dd3ea7-3b84-4b78-9310-0581d1d7534b.png">

* * *

### Recurrent Neural Model(RNN)
* 자기 자신으로 돌아오는 구조의 모델

<img width="578" alt="image" src="https://user-images.githubusercontent.com/93971443/194324625-baf21d08-460c-4266-96c4-52fa1eb7a8c4.png">

* 시간 순으로 펼치게 되면 입력이 굉장히 많은 fully connected layer과 같음
* 단점
  - 먼 과거의 정보가 잘 반영되지 않음
  - sigmoid 사용시 gradient vanishing 발생
  - ReLU 사용시 gradient exploding 발생

* * *

### LSTM(Long Short Term Memory()
* vanilla RNN의 단점을 보완

<img width="597" alt="image" src="https://user-images.githubusercontent.com/93971443/194325627-18d659eb-74e1-4f5c-969c-b17220f8bc44.png">

* 여러 개의 input을 받음(previous cell state, previous hidden state, 해당 시점 input)
* gate 종류 및 역할
  - Forget : 들어온 정보 중 어떤 정보를 버릴지 결정
  - Input : 어떤 정보를 cell state에 저장할지 결정, sigmoid로 정한 후 tanh가 Ctilda vector를 만듦
  - Update : forget, input gate에서 나온 정보들을 과거 정보와 취합
  - Output : 무엇을 output으로 보낼지 결정 후 내보냄

* * *

### GRU(Gated Recurrent Unit)
* 적은 parameter 수로 다른 것과 동일한 output을 낼 경우 generalize performance good에서 착안
* LSTM 구조에서 reset(==forget), update gate만 있는 구조
* cell state는 없고 hidden cell state만 있음(== output)

### ※ 위 내용 및 이미지는 네이버 부스트캠프 교육 자료를 참고하였습니다.




