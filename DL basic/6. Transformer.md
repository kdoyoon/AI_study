# Transformer

* * *
* attention에 기반한 모델로 RNN, CNN 모델보다 큰 데이터를 한번에 처리할 수 있음
<img width="467" alt="image" src="https://user-images.githubusercontent.com/93971443/194798578-6f406708-80ac-45d8-ab13-aa68160a805c.png">

* 왼쪽이 Encoder, 오른쪽이 Decoder 역할
* 재귀적 구조 X

* * *

### Encoder
* Encoder 구조(self-attention + MLP)
* 여러 층의 Encoder로 이루어져있음
<img width="792" alt="image" src="https://user-images.githubusercontent.com/93971443/194799448-9a14a887-785f-42b8-be39-46037c85639e.png">

* embedding 된 단어가 self-attention으로 들어가 연산 진행
* self-attention 부분에서는 각 단어에 다른 모든 단어가 영향을 줌(dependencies)
* MLP는 단순 변환이라 생각하면 됨(independent)
<img width="626" alt="image" src="https://user-images.githubusercontent.com/93971443/194801544-b01a44ab-9a17-405c-8920-c45bf39328ba.png">

#### self-attention
* 해당 단어가 문장내 다른 단어와 어떤 interation이 있는지 파악
* Query, Key, Value 세개의 벡터 생성(= embedding)
* 계산 과정
  - 한 단어의 Query 벡터와 다른 단어(자신꺼 포함)의 Key 벡터와의 내적을 통해 score 계산
  - score을 Key 벡터 차원의 제곱근( $\sqrt{d_k}$ )으로 나눔
  - softmax 함수을 통한 normalization
  - Value 벡터와 weighted sum(가중치 연산_연관성 없는 단어 희미해짐)

<img width="635" alt="image" src="https://user-images.githubusercontent.com/93971443/194803009-090f76be-e1c5-4b36-bd0b-5f6e10d25aa6.png">

#### MHA(Multi-Head-Attention)
* attention을 여러개 사용
* n개의 head있을 시 n개의 encoded vector기 나옴
* vector들을 concat한 후 weigted matrix를 곱해 차원을 줄여줌   
   
* 위 과정을 거친 후 positional encoding을 진행해줌   
(※ 하는 이유 : transformer가 순서에 independent하기 때문에 순서 정보를 주기 위해 필요)

* * *
### Decoder
* Decoder에서 generation할 때 auto-regressive하게 한 단어씩 나옴
* Encoder에서 input의 Key, Value 벡터를 받음   
(※ k, v 벡터 받는 이유 : 출력하고자 하는 단어의 attention map을 만들기 위해선 input의 k, v 벡터가 필요)

* 추론 시에는 미래 시점의 값을 알 수 없으니(학습해서 알고 있지만) 앞서 decoding된 단어에 대해서만 의존하여 추론 진행   


### ※ 위 내용 및 이미지는 네이버 부스트캠프 교육 자료를 참고하였습니다.
