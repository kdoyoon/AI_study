# 파이토치 문제해결

* * *
### OOM(Out Of Memeory)
- 왜 발생했는지 알기 어려움
- 어디서 발생했는지 알기 어려움
- Error backtracking이 이상한데로 감
- 메모리의 이전 상황 파악 어려움

##### 간단한 해결 방법
- batch size 줄임
- GPU를 비움(colab의 경우 run 초기화 후 다시 실행)

* * *
### GPUUtil 사용하기
- GPU 상태를 보여주는 모듈
- colab 환경에서 GPU 상태 보여주기 편함
- iter마다 메모리가 늘어나는지 확인(-> 늘어난다면 Memory가 세어나가는 상황)

```python
!pip install GPUtil

import GPUtil
GPUtil.showUtilization()
```

### torch.cuda.empty_cache() 사용
- 사용되지 않은 GPU상 cache를 정리
- 가용 메모리를 확보
- del 과는 구분이 필요(del : 관계를 끊음)
- reset 대신 쓰기 좋은 함수
- loop 들어가기 전 실행해주면 메모리를 충분히 확보한 채로 시작할 수 있음

```python
import torch
from GPUtil import showUtilization as gpu_usage

print("Initial GPU Usage")  
gpu_usage()

tensorList = []  
for x in range(10):  
tensorList.append(torch.randn(10000000,10).cuda())

print("GPU Usage after allcoating a bunch of Tensors")  
gpu_usage()

# del에서 확보한 메모리는 garbage collector가 실행되어야 사용할 수 있음
del tensorList  
print("GPU Usage after deleting the Tensors")  
gpu_usage()

#empty_cache : garbage collctor를 강제적으로 실행시켜줌
print("GPU Usage after emptying the cache")  
torch.cuda.empty_cache()  
gpu_usage()
```

### training loop에 tensor로 축적되는 변수는 확인할 것

- tensor로 처리된 변수는 GPU 상에 메모리 사용
- 해당 변수 loop 안에 연산에 있을 때 GPU에 computational graph를 생성(메모리 잠식)

###### 해결책
- 1-d tensor의 경우 python 기본 객체로 변환하여 처리
- item_loss.item() 또는 float(item_loss)와 같은 방법 사용

### del 명령어를 적절히 사용하기
- 필요가 없어진 변수는 적절한 삭제가 필요함
- python의 메모리 배치 특성상 loop이 끝나도 메모리를 차지함


### 가능 batch 사이즈 실험해보기
- 학습 시 OOM이 발생했다면 batch 사이즈를 1로(1은 아니더라도 사이즈를 줄여서)해서 실험

### torch.no_grad() 사용하기
- inference 시점에서는 torch.no_grad() 구문 사용
- backward pass으로 인해 쌓이는 메모리에서 자유로움(-> 메모리 버퍼가 안일어남)
- context안에서 backward가 일어나지 않음


### 예상치 못한 에러 메시지
- CUDNN_STATUS_NOT_INIT이나 device-side-assert 등 발생
- cuda와 관련한 OOM의 일종으로 생각될 수 있고, 적절한 코드 처리 필요

### 그 외
- colab에서 너무 큰 사이즈는 실행 X
- torchsummary를 사용하여 데이터 input 사이즈 맞출 것
- tensor의 float precision을 16bit로 줄일 수 있음


### ※ 위 내용 및 이미지는 네이버 부스트캠프 교육 자료를 참고하였습니다.
