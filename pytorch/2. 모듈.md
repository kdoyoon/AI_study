# 모듈

* * *

### nn.module
- 여러 기능을 한 곳에 모아두는 상자 역할
- 안에서 여러 함수를 정의하기도 하고 다른 모듈을 넣을 수도 있음
<pre>
<code>
import torch.nn as nn
import torch.nn.functional as F

class Mode(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1 = nn.Conv2d(1, 20, 5)
    self.conv2 = nn.Conv2d(20, 20, 5)
  
  def forward(self, x):
    x = F.relu(self.conv1(x))
    return F.relu(self.conv2(x))
</code>
</pre>

#### nn.Linear VS. nn.LazyLinear
- Linear
  - 미리 입력값과 출력값의 크기를 지정해야함
  - torch.nn.parameter 사용
  - layer 생성 시에 parameter(W, b) 초기화

- LazyLinear
  - 출력값의 크기만 지정
  - 입력값의 크기는 첫 forward 진행할 때 자동으로 확인하여 지정
  - torch.nn.UninitializedParameter 사용
  - layer 생성 후 첫 forward 진행 중에 parameter(W,b) 초기화 


※ super().__init__()의 역할
-부모 클래스를 초기화, 속성을 받옴(초기화 안할 속성 사용 불가)

#### 종류
- Sequential
  - 모듈을 하나로 묶어 순차적으로 실행시키고 싶을 때 사용
- ModuleList
  - list처럼 모아둔 후 원하는 것을 인덱싱을 이용하여 사용
- ModuleDict
  - dict처럼 key 값을 이용하여 호출

* List VS. ModuleList
  * List
    * 일반 list는 모듈 내에서 파라미터로 작동하지 않기 때문에 저장되지 않는다.
    * list 및 list에 담아놓은 모듈들 전체가 nn.Module의 submodule로 등록이 안됨
  * Modulelist
    * 파라미터로 사용이 되기 때문에 모듈에 저장이 된다.
    * ModuleList에 담아놓은 모듈들 전체가 nn.Module의 submodule로 등록됨

#### 딥러닝의 흐름
function들이 모여 layer을 이루고, layer이 모여 Module이 된다. 이 모듈들이 모이면 딥러닝 모델이 됨

* * *

### Parameter

모듈에서 사용되는 weight와 bias와 같은 정보를 갖는 텐서

* 왜 일반 tensor를 안쓰고 parameter를 쓸까?
  * 모듈 내에서 파라미터로 사용되어야 저장이 가능하고 이를 backward 연산시 활용할 수 있다.
  * 일반 tensor는 값이 업데이트 되지 않고, 모델 저장시에도 무시됨

#### Buffer
* 파라미터가 아닌 텐서를 저장하고 싶을 때 사용
* register_buffer를 통해 저장

#### tensor / parameter / buffer
* tensor
  * gradient 계산 X
  * 값 업데이터 X
  * 모델 저장시 값 저장 X
* parameter
  * gradient 계산 O
  * 값 업데이터 O
  * 모델 저장시 값 저장 O
* buffer
  * gradient 계산 X
  * 값 업데이터 X
  * 모델 저장시 값 저장 O
  * register_buffer()통해 등록
 
 ### named_children, named_modules, named_parameters
 * children : 한 단계 아래의 sub module까지만 표시
 * modules : 자신에게 속하는 모든 submodule 표시
 * parameters : 모듈의 파라미터 표시
 
 * * *
 ##### extra_repr
 <pre>
 <code>
 def extra_repr(self):
        # (Optional)Set the extra information about this module. You can test
        # it by printing an object of this class.
        return 'input_features={}, output_features={}, bias={}'.format(
            self.input_features, self.output_features, self.bias is not None
        )
 </code>
 </pre>
 
* _repr_ : repr()로 _repr_ 메소드를 호출, formal string(''로 감싸진 문자열)출력 / 해당 객체를 인간이 이해할 수 있게 출력
* extra_repr : 모듈에 대한 추가 정보를 전달  
 * * *
 
 ### hook, apply
 
 #### hook
 - 프로그램의 실행 로직을 분석
 - 프로그램에 추가적인 기능을 제공하고 싶을 때 사용
 - tensor hook
  - backward hook
 - module hook
  - forward
  - backward
 - <pre><code>__dict__</code></pre> 를 이용하여 생성된 hook 확인 가능

<pre>
<code>
class Package(object):
  def __init__(self):
    self.programs = [program_A, program_B]
    self.hooks = [] # 여기에 내가 원하는 함수를 넣어서 사용하는듯
  
  def __call__(self, x):
    for program in self.programs:
      x = program(x)
      # 이 코드를 추가해서 모델 실행 중간에 내가 원하는 함수를 적용할 수 있음
      if self.hooks:
        for hook in self.hooks:
          output = hook(x)
          
          if output:
            x = output
</code>
</pre>
 * [모듈 hook에 대한 내용](https://velog.io/@naem1023/pytorch-hook) 


#### apply
- Model을 구성하는 모든 module에 동일한 함수를 적용
- pre-trained 된 모듈에 내가 만든 custom 함수를 적용하고 싶을 때 사용
- 모듈을 입력으로 받음
- 가중치 초기화에 자주 사용
- postorder 방식으로 적용



### ※ 위 내용 및 이미지는 네이버 부스트캠프 교육 자료를 참고하였습니다.
