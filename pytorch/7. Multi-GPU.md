# Mulyi-GPU

* * *

### 개념
- single : 1개 / Multi : 2개 이상
- Node : 한 대의 컴퓨터
- GPU $⊂$ Node

* * *

### Model Parallel

- GPU 분산(2가지)
  - 모델 parallel
    - 과거부터 사용함_alex_net
    - 모델의 병목현상과 파이프라인 구성의 어려움
```python
class ModelParallelResNet50(ResNet):
  def __init__(self, *args, **kwargs):
    super(ModelParallelResNet50).__init__(
    Bottleneck, [3, 4, 6, 3], num_classes = num_classes, *args, *kwargs)
    
    # 첫번째 모델을 cuda0에 할당
    self.seq1 = nn.Sequential(self.conv1, self. bn1, self.relu, self.maxpool,
    self.layer1, self.layer2).to('cuda:0')
    
    # 두번째 모델을 cuda1에 할당
    self.seq2 = nn.Sequential(
      self.layer3, self.layer4, self.avgpool).to("cuda:1")
    
     def forward(self, x):
      x = self.seq2(self.seq1(x).to('cuda:1'))
      return self.fc(x.view(x.size(0),-1))
```
  - 데이터 parallel
    - 말 그대로 데이터를 나눠 GPU에 할당한 후 결과의 평균을 취함
    - mini-batch와 유사하지만 한번에 여러 GPU에서 진행

  - 2가지 방법
    - DataParallel : 단순히 데이터를 분배한 후 평균을 취함 -> GPU 사용 불균형 문제 발생(병목)
    - DistributedDataParallel : 각 CPU마다 process생성하여 개별 GPU에 할당
      - 기본적으로 DataParallel로 개별적 연산을 진행
```python
############DataParallel#############
parallel_model = torch.nn.DataParallel(model)

# 학습과정
predictions = parallel_model(inputs)
loss = loss_function(predictions, labels)
loss.mean().backward()
optimizer.step()

#위 과정 반복

############DistributedDataParallel#############
model =torch.nn.DistributedDataParallel(model)
train_sampler = troch.utils.data.distributed.DistributedDataParallel(train_Data)
```
 
### ※ 위 내용 및 이미지는 네이버 부스트캠프 교육 자료를 참고하였습니다.
